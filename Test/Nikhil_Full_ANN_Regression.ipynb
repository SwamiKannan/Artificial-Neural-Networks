{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!conda install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-0067cfc428cd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To-do\n",
    "&nbsp;&nbsp; 1. Pull in the data (This includes distance calculation) <br>\n",
    "&nbsp;&nbsp; 2. Run around with the categorical data and extract data from timestamped details <br>\n",
    "&nbsp;&nbsp; 3. Do basic exploratory analysis <br>\n",
    "&nbsp;&nbsp; 4. Create categorical data matrix and continuous data matrix <br>\n",
    "&nbsp;&nbsp; 5. Create tensors and embedding sizes for categorical data <br>\n",
    "&nbsp;&nbsp; 6. Create model, optimizer and criterion <br>\n",
    "&nbsp;&nbsp; 7. Build train model <br>\n",
    "&nbsp;&nbsp; 8. Run test <br>\n",
    "&nbsp;&nbsp; 9. Save model, load model <br>\n",
    "&nbsp;&nbsp;10. Create program to directly run data transformation and through model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Pull in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('..//Data//NYCTaxiFares.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def haversine_distance(df, lat1, long1, lat2, long2):\n",
    "    \"\"\"\n",
    "    Calculates the haversine distance between 2 sets of GPS coordinates in df\n",
    "    \"\"\"\n",
    "    r = 6371  # average radius of Earth in kilometers\n",
    "       \n",
    "    phi1 = np.radians(df[lat1])\n",
    "    phi2 = np.radians(df[lat2])\n",
    "    \n",
    "    delta_phi = np.radians(df[lat2]-df[lat1])\n",
    "    delta_lambda = np.radians(df[long2]-df[long1])\n",
    "     \n",
    "    a = np.sin(delta_phi/2)**2 + np.cos(phi1) * np.cos(phi2) * np.sin(delta_lambda/2)**2\n",
    "    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1-a))\n",
    "    d = (r * c) # in kilometers\n",
    "\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['distance_km']=haversine_distance(df,'pickup_latitude','pickup_longitude','dropoff_latitude','dropoff_longitude')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Run around with the categorical data and extract data from timestamped details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['pickup_datetime']=pd.to_datetime(df['pickup_datetime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['pickup_datetime'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting EST to NewYork time since all the data is taken from NYC\n",
    "df['date_timeEST']=df['pickup_datetime']-pd.Timedelta(hours=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating various columns that are extracted data from the timestamp\n",
    "df['hours']=df['date_timeEST'].dt.hour\n",
    "df['weekday']=df['date_timeEST'].dt.strftime('%a')\n",
    "df['AM_PM']=np.where(df['hours']<12,\"AM\",\"PM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cont=df.copy()\n",
    "df_cont.drop(['pickup_datetime','pickup_longitude','pickup_latitude','dropoff_longitude','dropoff_latitude','fare_class','hours','weekday','AM_PM'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#correlation of all significant continuous variables\n",
    "df_cont.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Distribution of data of all continuous variables\n",
    "for cont in df_cont.columns:\n",
    "    plt.hist(df[cont])\n",
    "    plt.xlabel('X-Axis')\n",
    "    plt.ylabel('Y-Axis')\n",
    "    plt.title(cont)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cat=df.copy()\n",
    "df_cat.drop(['fare_amount','passenger_count','distance_km'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Distribution of data of all continuous variables\n",
    "for cat in df_cat.columns:\n",
    "    plt.hist(df[cat])\n",
    "    plt.xlabel('X-Axis')\n",
    "    plt.ylabel('Y-Axis')\n",
    "    plt.title(cat)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Create categorical data matrix and continuous data matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating an index of all categorical columns in index\n",
    "cats=['hours','weekday','AM_PM']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To use [category dtype].cat.codes, convert all non continuous objects to 'category' codes\n",
    "for cat in cats:\n",
    "    df[cat]=df[cat].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a matrix of all categorical columns of all int index values for each category \n",
    "#e.g. weekdays will have 7 codes (0-6), hours will have 24 codes (0-23)\n",
    "cat_cols=np.stack([df[cat].cat.codes for cat in cats],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating index of continuous columns\n",
    "conts=['pickup_longitude',\n",
    "       'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude',\n",
    "       'passenger_count', 'distance_km']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cont_cols=np.stack([df[col].values for col in conts],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label=df['fare_amount'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('category shape',cat_cols.shape)\n",
    "print('continuous shape',cont_cols.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.Create tensors and embedding sizes for categorical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_tensor=torch.tensor(cat_cols,dtype=torch.long)\n",
    "cont_tensor=torch.tensor(cont_cols,dtype=torch.float)\n",
    "label_tensor=torch.tensor(label,dtype=torch.float)\n",
    "print('category shape',cat_tensor.shape)\n",
    "print('continuous shape',cont_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5a. Creating the embedding object\n",
    "&nbsp;&nbsp; 1.  Creating embedding sizes : \n",
    "We need to create an embedding matrix of dimensions [original no.of cats, new numbers of cats] This is because we use only one-hotkey encoding, then the dimensions of each vector will by original no. of unique values of the categorical column which will be very large. Hence, we want to squeeze this to a more reasonable number of dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_len=[df[col].nunique() for col in cats]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size=[(cat,min(50,int((cat+1)//2))) for cat in cat_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_embeddings = [nn.Embedding(initial,target) for initial,target in embed_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5b. Trying to create an embedding matrix for the sample to see how it goes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_sample=torch.tensor(cat_cols[:4,:],dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cat_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Attempting only for the hours data - to convert a coded 24-dimension feature into an embedding\n",
    "embed_hour=cat_embeddings[0]\n",
    "embed_hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test sample of hours data\n",
    "test_hour=torch.tensor(cat_sample[:,0],dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(embed_hour(test_hour))\n",
    "#We see that a 24 dimension variable now has only 12, which is a victory!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now attempting the same for all features in cat_sample\n",
    "embeddings_test=[]\n",
    "for i, embed in enumerate(cat_embeddings):\n",
    "    embeddings_test.append(embed(cat_sample[:,i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,e in enumerate(embeddings_test):\n",
    "    print(cats[i])\n",
    "    print(e.shape)\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### SUCCESS ON SAMPLE!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.Create model, optimizer and criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cont_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self,cat_cols,cont_cols,layer_count,output_features,embed_size_list,p=0.5): \n",
    "        '''\n",
    "        args:\n",
    "        cat_cols is the tensor of all categorical values (pre-embedding)\n",
    "        n_cont=number of continuous variables (for batch normalization)\n",
    "        cont_cols is the tensor of all continuous values\n",
    "        input_features - number of parameters of input\n",
    "        layer_count - a tuple of number of nodes of each hidden layer\n",
    "        output_features = number of outputs expected\n",
    "        embed_size_list is list of embedding sizes for the categorical values\n",
    "        p = basically, the % of nodes to be nullified during dropout layer\n",
    "       \n",
    "       Approach: In the constructor, create all the layers (Linear, ReLU, Batch and Dropout) for each hidden layer as per layer_count)\n",
    "       and add them to sequential(). Fwd() will have all the data manipulation and final embedding\n",
    "       \n",
    "        '''\n",
    "        super().__init__()\n",
    "        #self.input_features=input_features\n",
    "        self.output_features=output_features\n",
    "        self.layer_count=layer_count\n",
    "        self.embed_size_list=embed_size_list\n",
    "        #Create embeddings from categorical columns as seen in the test case\n",
    "        self.embeddings=[nn.Embedding(base_dim, target_dim) for base_dim, target_dim in embed_size_list]\n",
    "#         print(self.embeddings)\n",
    "        n_cont=cont_cols.shape[1]\n",
    "        self.batch_norm=nn.BatchNorm1d(n_cont)\n",
    "        self.dropout=nn.Dropout(p=0.5)\n",
    "        #To create the layers we need to start with input sizes\n",
    "        n_cont=cont_cols.shape[1]\n",
    "        #We need to find the total number of columns in the data. \n",
    "        #cat_cols currently only has 4 columns which post embedding will go to 23 \n",
    "        n_in=sum(nf for ni,nf in self.embed_size_list)+n_cont\n",
    "#         print('n_in',n_in)\n",
    "        self.layers=[]\n",
    "        for l in self.layer_count:\n",
    "            self.layers.append(nn.Linear(n_in,l))\n",
    "            self.layers.append(nn.ReLU(inplace=True))\n",
    "            self.layers.append(nn.Dropout(p))\n",
    "            self.layers.append(nn.BatchNorm1d(l))            \n",
    "            n_in=l\n",
    "        self.layers.append(nn.Linear(self.layer_count[-1],self.output_features))\n",
    "#         print(self.layers)\n",
    "        self.final_layers=nn.Sequential(*self.layers)\n",
    "#         print(self.final_layers)\n",
    "        \n",
    "    def forward(self,cat_cols, cont_cols):\n",
    "        '''\n",
    "        1. Create the embedding for cat and create one final input value for the forward path (only one time this gets created)\n",
    "        2. Create the dropout for this input layer\n",
    "        3. Create a batch norm for this layer\\\n",
    "        4. Pass this into the all the layers moving forward (starting with the first linear layer)\n",
    "        '''\n",
    "        #Creating the embedding for categorical columns\n",
    "        embeds=[]\n",
    "        for i,e in enumerate(self.embeddings):\n",
    "#             print('e' ,e)\n",
    "            embeds.append(e(cat_cols[:,i]))\n",
    "        cat_final=torch.cat(embeds,axis=1)\n",
    "#         print(cat_final.shape) #17 columns cos 12 for hours, 2 for AM/PM and 4 for days post embedding\n",
    "#         print(cont_cols.shape) # 6 columns one for each of the 6 features\n",
    "        \n",
    "        #Batch normalize the continuous variables first\n",
    "        cont_cols=self.batch_norm(cont_cols)\n",
    "        cat_final=self.dropout(cat_final)\n",
    "        self.X=torch.cat((cat_final, cont_cols),axis=1)\n",
    "#         print('X shape', self.X.shape)\n",
    "#         print('X dtype',self.X.dtype)\n",
    "#         print('X class',type(self.X))\n",
    "        #Dropout for the complete data set\n",
    "        #self.X=self.dropout(self.X)\n",
    "        self.X=self.final_layers(self.X)\n",
    "        return self.X\n",
    "            \n",
    "\n",
    "        #Creating final data set with cat and cols\n",
    "          \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(33)\n",
    "model = Model(cat_tensor,cont_tensor,layer_count=[200,100],output_features=1,embed_size_list=embed_size,p=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing if forward() works\n",
    "z = model.forward(cat_tensor,cont_tensor)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion=nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer=torch.optim.Adam(model.parameters(),lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.Create train, test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kaggle data set says that the data is already shuffled. If the data is not shuffled, then we need to do a train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size=int(0.2*df.shape[0])\n",
    "print('test size',test_size)\n",
    "print('train size',df.shape[0]-test_size)\n",
    "cat_cols\n",
    "X_train_cont=cont_tensor[:test_size,:]\n",
    "X_train_cat=cat_tensor[:test_size,:]\n",
    "X_test_cont=cont_tensor[test_size:,:]\n",
    "X_test_cat=cat_tensor[test_size:]\n",
    "label_train=label_tensor[:test_size].reshape(-1,1)\n",
    "label_test=label_tensor[test_size:].reshape(-1,1)\n",
    "print('X_train_cont',X_train_cont.shape)\n",
    "print('X_train_cat',X_train_cat.shape)\n",
    "print('X_test_cont',X_test_cont.shape)\n",
    "print('X_test_cat',X_test_cat.shape)\n",
    "print('label_train',label_train.shape)\n",
    "print('label_test',label_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Building the train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=300\n",
    "losses3=[]\n",
    "for e in range(epochs):\n",
    "    e+=1\n",
    "    y_pred=model.forward(X_train_cat,X_train_cont)\n",
    "    loss=criterion(y_pred,label_train)**0.5\n",
    "    losses3.append(loss)\n",
    "    if e%10==0:\n",
    "        print(f' Loss at epoch {e} is {loss}')\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(epochs),losses,'g')\n",
    "plt.plot(range(epochs),losses1,'r')\n",
    "plt.plot(range(epochs),losses3,'b')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Losses')\n",
    "plt.title('Loss for lr=0.1 (red) and lr=0.001 (green)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. Running on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    y_preds=model.forward(X_test_cat,X_test_cont)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_test=criterion(y_preds, label_test)\n",
    "loss_test**0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10. Analysing the error spread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error=(y_preds-label_test).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(pd.DataFrame(error).describe())\n",
    "print('Mean of error is {:8.4f}'.format(error.mean() ))\n",
    "print('Std dev of error is {:8.4f}'.format(error.std()))\n",
    "print('Range of error is {:8.4f}'.format(error.max()-error.min()))\n",
    "print('Max of error is {:8.4f} at sample {}'.format(error.max(), (error.argmax())))\n",
    "print('Max of error is {:8.4f} at sample {}'.format(error.min(), (error.argmin())))\n",
    "plt.hist(error)\n",
    "plt.title('Distribution of the error variable for test case')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_std_dev_p=error.mean()+error.std()\n",
    "one_std_dev_n=error.mean()-error.std()\n",
    "ranges=((error<=one_std_dev_p) & (error>=one_std_dev_n)).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('% of error within one standard deviation on either side is: {:8.3f}%'.format(ranges/len(error)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11. Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(losses)==epochs:\n",
    "    torch.save(model.state_dict(),'uber_model_weights.pt')\n",
    "    torch.save(model,'uber_model.pkl')\n",
    "else:\n",
    "    print(\"You haven't trained this model! Only trained models should be saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 12. Create program to directly run data transformation and through model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Refer Nikhil_Full_ANN_prediction_operation.ipynb </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
